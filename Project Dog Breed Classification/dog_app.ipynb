{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "dog_app.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishilesh/Udacity_DLND/blob/master/Project%20Dog%20Breed%20Classification/dog_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxsLcaExAaP-",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks\n",
        "\n",
        "## Project: Write an Algorithm for a Dog Identification App \n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
        "\n",
        "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to **File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
        "\n",
        "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
        "\n",
        ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
        "\n",
        "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "### Why We're Here \n",
        "\n",
        "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
        "\n",
        "![Sample Dog Output](images/sample_dog_output.png)\n",
        "\n",
        "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
        "\n",
        "### The Road Ahead\n",
        "\n",
        "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
        "\n",
        "* [Step 0](#step0): Import Datasets\n",
        "* [Step 1](#step1): Detect Humans\n",
        "* [Step 2](#step2): Detect Dogs\n",
        "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "* [Step 4](#step4): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
        "* [Step 5](#step5): Write your Algorithm\n",
        "* [Step 6](#step6): Test Your Algorithm\n",
        "\n",
        "---\n",
        "<a id='step0'></a>\n",
        "## Step 0: Import Datasets\n",
        "\n",
        "Make sure that you've downloaded the required human and dog datasets:\n",
        "* Download the [dog dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip).  Unzip the folder and place it in this project's home directory, at the location `/dogImages`. \n",
        "\n",
        "* Download the [human dataset](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip).  Unzip the folder and place it in the home directory, at location `/lfw`.  \n",
        "\n",
        "*Note: If you are using a Windows machine, you are encouraged to use [7zip](http://www.7-zip.org/) to extract the folder.*\n",
        "\n",
        "In the code cell below, we save the file paths for both the human (LFW) dataset and dog dataset in the numpy arrays `human_files` and `dog_files`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKJfOYcgAhWy",
        "colab_type": "code",
        "outputId": "84945d8c-203c-47d3-ddd0-4d7a113c4a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget -c https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
        "!wget -c https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n",
        "!unzip -qq dogImages.zip\n",
        "!mv dogImages dog_images\n",
        "!unzip -qq lfw.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-08 11:20:34--  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.24.37\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.24.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1132023110 (1.1G) [application/zip]\n",
            "Saving to: ‘dogImages.zip’\n",
            "\n",
            "dogImages.zip       100%[===================>]   1.05G  21.0MB/s    in 52s     \n",
            "\n",
            "2019-12-08 11:21:27 (20.7 MB/s) - ‘dogImages.zip’ saved [1132023110/1132023110]\n",
            "\n",
            "--2019-12-08 11:21:31--  https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.116.136\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.116.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196739509 (188M) [application/zip]\n",
            "Saving to: ‘lfw.zip’\n",
            "\n",
            "lfw.zip             100%[===================>] 187.62M  20.4MB/s    in 10s     \n",
            "\n",
            "2019-12-08 11:21:42 (18.6 MB/s) - ‘lfw.zip’ saved [196739509/196739509]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sByB6SahAaQC",
        "colab_type": "code",
        "outputId": "29a7e792-7675-4dc1-c21e-0630fffc2228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"/content/lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"/content/dog_images/*/*/*\"))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print('There are %d total human images.' % len(human_files))\n",
        "print('There are %d total dog images.' % len(dog_files))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 13233 total human images.\n",
            "There are 8351 total dog images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rwTZecAAaQI",
        "colab_type": "text"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## Step 1: Detect Humans\n",
        "\n",
        "In this section, we use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  \n",
        "\n",
        "OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBsOf8nAaQJ",
        "colab_type": "code",
        "outputId": "69085718-7dd1-4d9e-a747-cc9666539696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import cv2                \n",
        "import matplotlib.pyplot as plt                        \n",
        "%matplotlib inline                               \n",
        "\n",
        "# extract pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_alt.xml')\n",
        "\n",
        "# load color (BGR) image\n",
        "img = cv2.imread(human_files[0])\n",
        "# convert BGR image to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# find faces in image\n",
        "faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "# print number of faces detected in the image\n",
        "print('Number of faces detected:', len(faces))\n",
        "\n",
        "# get bounding box for each detected face\n",
        "for (x,y,w,h) in faces:\n",
        "    # add bounding box to color image\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    \n",
        "# convert BGR image to RGB for plotting\n",
        "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# display the image, along with bounding box\n",
        "plt.imshow(cv_rgb)\n",
        "plt.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of faces detected: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKacLJnBBF/QppwsmcEEX9Cmn/w9oqQaWtDcmFgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wMypQ8NAaQO",
        "colab_type": "text"
      },
      "source": [
        "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
        "\n",
        "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
        "\n",
        "### Write a Human Face Detector\n",
        "\n",
        "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRI7cEf_AaQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns \"True\" if face is detected in image stored at img_path\n",
        "def face_detector(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    return len(faces) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDxK9T2vAaQV",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Assess the Human Face Detector\n",
        "\n",
        "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
        "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
        "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
        "\n",
        "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvlR6TYQAaQX",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ \n",
        "(You can print out your results and/or write your percentages in this cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwsGYgGoAaQY",
        "colab_type": "code",
        "outputId": "f5bce2f9-6d62-4e8a-8815-0f0a66203019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "human_files_short = human_files[:100]\n",
        "dog_files_short = dog_files[:100]\n",
        "\n",
        "#-#-# Do NOT modify the code above this line. #-#-#\n",
        "\n",
        "## TODO: Test the performance of the face_detector algorithm \n",
        "## on the images in human_files_short and dog_files_short.\n",
        "h = 0\n",
        "d = 0\n",
        "for i in human_files_short:\n",
        "  if(face_detector(i)):\n",
        "    h+=1\n",
        "for i in dog_files_short:\n",
        "  if(face_detector(i)==False):\n",
        "    d+=1\n",
        "print(\"No. of Humans detected: \",h)\n",
        "print(\"No. of Dogs detected: \",d)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-0da76b0d89a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdog_files_short\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No. of Humans detected: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-1d0612fb4078>\u001b[0m in \u001b[0;36mface_detector\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqjkDTsdAaQc",
        "colab_type": "text"
      },
      "source": [
        "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on `human_files_short` and `dog_files_short`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAa3LJUSAaQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### (Optional) \n",
        "### TODO: Test performance of another face detection algorithm.\n",
        "### Feel free to use as many code cells as needed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikpYahjHAaQg",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Detect Dogs\n",
        "\n",
        "In this section, we use a [pre-trained model](http://pytorch.org/docs/master/torchvision/models.html) to detect dogs in images.  \n",
        "\n",
        "### Obtain Pre-trained VGG-16 Model\n",
        "\n",
        "The code cell below downloads the VGG-16 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALuYiRskAaQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# define VGG16 model\n",
        "VGG16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# move model to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    VGG16 = VGG16.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw2ffmCEAaQk",
        "colab_type": "text"
      },
      "source": [
        "Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMqlcDqfAaQl",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
        "\n",
        "In the next code cell, you will write a function that accepts a path to an image (such as `'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'`) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.\n",
        "\n",
        "Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the [PyTorch documentation](http://pytorch.org/docs/stable/torchvision/models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aRDf-WZAaQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set PIL to be tolerant of image files that are truncated.\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "image_size = 224\n",
        "MEANS = [0.485, 0.456, 0.406]\n",
        "DEVIATIONS = [0.229, 0.224, 0.225]\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(MEANS,DEVIATIONS)\n",
        "                                ])\n",
        "\n",
        "def VGG16_predict(img_path):\n",
        "    '''\n",
        "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
        "    predicted ImageNet class for image at specified path\n",
        "    \n",
        "    Args:\n",
        "        img_path: path to an image\n",
        "        \n",
        "    Returns:\n",
        "        Index corresponding to VGG-16 model's prediction\n",
        "    '''\n",
        "    \n",
        "    ## TODO: Complete the function.\n",
        "    ## Load and pre-process an image from the given img_path\n",
        "    ## Return the *index* of the predicted class for that image\n",
        "    \n",
        "   \n",
        "    img = Image.open(str(img_path)) # OPEN IMAGE\n",
        "    img = transform(img).unsqueeze(0) # TRANSFORM THE IMAGE\n",
        "    output = VGG16(img) # PREDICT\n",
        "    # prob = torch.exp(output)  # CALCULATE THE PROBABILITIES\n",
        "    # top_prob,top_class = prob.topk(1,dim=1) # FIND THE BEST\n",
        "    index = output.data.numpy().argmax()\n",
        "    #return top_class.item() # RETURN\n",
        "    return index\n",
        "    \n",
        "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # img = transform(gray)\n",
        "    # img = img.reshape((3,300,192))\n",
        "    #print(output)\n",
        "    #index = VGG16(img)\n",
        "    #cv_rgb = cv2.cvtColor(gray, cv2.COLOR_BGR2RGB)\n",
        "    # plt.imshow(img)\n",
        "    # plt.show()\n",
        "    #print(index)\n",
        "    #return index # predicted class index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OPWuVeDAaQq",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Write a Dog Detector\n",
        "\n",
        "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).\n",
        "\n",
        "Use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0320gXNiAaQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### returns \"True\" if a dog is detected in the image stored at img_path\n",
        "def dog_detector(img_path):\n",
        "    ## TODO: Complete the function.\n",
        "    ind = VGG16_predict(img_path) # Calculation of the index\n",
        "    \n",
        "    if(ind>=151 and ind<=268): # Checking if its Dog/Human\n",
        "      return True\n",
        "    return False # true/false"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I_17qCCAaQv",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Assess the Dog Detector\n",
        "\n",
        "__Question 2:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
        "- What percentage of the images in `human_files_short` have a detected dog?  \n",
        "- What percentage of the images in `dog_files_short` have a detected dog?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Z1CHfJAaQx",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x74xLxlcAaQx",
        "colab_type": "code",
        "outputId": "86492492-5be4-4a06-e0d7-c6d5ce1ab27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "### TODO: Test the performance of the dog_detector function\n",
        "### on the images in human_files_short and dog_files_short.\n",
        "d = 0\n",
        "for i in dog_files_short:\n",
        "  if(dog_detector(i)):\n",
        "    d+=1\n",
        "print(\"No. of Dogs: \",d)\n",
        "h = 0\n",
        "for i in human_files_short:\n",
        "  if(dog_detector(i)==False):\n",
        "    h+=1\n",
        "print(\"No. of Humans: \",h)\n",
        "#print(dog_detector(dog_files_short[0]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-141d062d03c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdog_files_short\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No. of Dogs: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-e0b33f4f3259>\u001b[0m in \u001b[0;36mdog_detector\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdog_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m## TODO: Complete the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculation of the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m151\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m268\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Checking if its Dog/Human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-273b5e64eb5c>\u001b[0m in \u001b[0;36mVGG16_predict\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# OPEN IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TRANSFORM THE IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# PREDICT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# prob = torch.exp(output)  # CALCULATE THE PROBABILITIES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# top_prob,top_class = prob.topk(1,dim=1) # FIND THE BEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV_iM9kEAaQ0",
        "colab_type": "text"
      },
      "source": [
        "We suggest VGG-16 as a potential network to detect dog images in your algorithm, but you are free to explore other pre-trained networks (such as [Inception-v3](http://pytorch.org/docs/master/torchvision/models.html#inception-v3), [ResNet-50](http://pytorch.org/docs/master/torchvision/models.html#id3), etc).  Please use the code cell below to test other pre-trained PyTorch models.  If you decide to pursue this _optional_ task, report performance on `human_files_short` and `dog_files_short`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpindQfpAaQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### (Optional) \n",
        "### TODO: Report the performance of another pre-trained network.\n",
        "### Feel free to use as many code cells as needed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAy4JyjBAaQ4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step3'></a>\n",
        "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
        "\n",
        "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
        "\n",
        "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
        "\n",
        "Brittany | Welsh Springer Spaniel\n",
        "- | - \n",
        "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
        "\n",
        "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
        "\n",
        "Curly-Coated Retriever | American Water Spaniel\n",
        "- | -\n",
        "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
        "\n",
        "\n",
        "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
        "\n",
        "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
        "- | -\n",
        "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
        "\n",
        "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
        "\n",
        "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
        "\n",
        "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively).  You may find [this documentation on custom datasets](http://pytorch.org/docs/stable/torchvision/datasets.html) to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of [transforms](http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64FFbw9_AaQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a809e0c5-2680-4c67-d6f2-e7d78badb950"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms,utils\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 20\n",
        "VALID_SIZE = 0.2\n",
        "### TODO: Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomRotation(15),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(\"/content/dog_images/train\",transform=transform)\n",
        "valid_data = torchvision.datasets.ImageFolder(\"/content/dog_images/valid\",transform=transform)\n",
        "test_data = torchvision.datasets.ImageFolder(\"/content/dog_images/test\",transform=transform)\n",
        "\n",
        "print(\"Number of Train data: \",str(len(train_data)))\n",
        "print(\"Number of Valid data: \",str(len(valid_data)))\n",
        "print(\"Number of Test data: \",str(len(test_data)))\n",
        "# num_train = len(train_data)\n",
        "# indices = list(range(num_train))\n",
        "# np.random.shuffle(indices)\n",
        "# split = int(np.floor(VALID_SIZE*num_train))\n",
        "# train_idx, valid_idx = indices[split:],indices[:split]\n",
        "\n",
        "# train_sampler = SubsetRandomSampler(train_idx)\n",
        "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size = BATCH_SIZE,num_workers=0,shuffle=True) \n",
        "valid_loader = torch.utils.data.DataLoader(valid_data,batch_size = BATCH_SIZE,num_workers=0,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size = BATCH_SIZE,num_workers=0,shuffle=True)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Train data:  6680\n",
            "Number of Valid data:  835\n",
            "Number of Test data:  836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQqEKpbkAaQ9",
        "colab_type": "text"
      },
      "source": [
        "**Question 3:** Describe your chosen procedure for preprocessing the data. \n",
        "- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n",
        "- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrImwmZUAaQ-",
        "colab_type": "text"
      },
      "source": [
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkW7IumCAaQ_",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Create a CNN to classify dog breed.  Use the template in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-xd3Q7X9vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.cuda.empty_cache()\n",
        "# !pip install torchsummary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dtDGxCBJLlr",
        "colab_type": "code",
        "outputId": "f4a034bf-987b-4bb9-9e45-8d8da7ba750b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# from PIL import ImageFile\n",
        "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# max_of_targets = -1\n",
        "\n",
        "# for b, (data, target) in enumerate(train_loader):\n",
        "#     max_val, max_ind = torch.max(target, 0)\n",
        "#     #     print(max_val)\n",
        "    \n",
        "#     if max_val > max_of_targets:\n",
        "#         max_of_targets = max_val\n",
        "#         print(max_of_targets)\n",
        "        \n",
        "# num_of_classes = max_of_targets.item()+1\n",
        "#num_of_classes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(123)\n",
            "tensor(132)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a03e0c0c6342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_of_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#     print(max_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \"\"\"\n\u001b[1;32m    686\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresized_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresized_crop\u001b[0;34m(img, i, j, h, w, size, interpolation)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img should be PIL Image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdE0JO3IAaRA",
        "colab_type": "code",
        "outputId": "829f85b2-518b-4869-8204-249ee6d6d61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    ### TODO: choose an architecture, and complete the class\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        ## Define layers of a CNN\n",
        "        # 20*256*256*3\n",
        "        self.conv1 = nn.Conv2d(3,16,3,padding=1)\n",
        "        # 20*254*254*16\n",
        "        self.conv2 = nn.Conv2d(16,64,3,padding=1)\n",
        "        # 20*252*252*64\n",
        "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
        "        #self.conv4 = nn.Conv2d(128,256,3,padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(128*28*28,500)\n",
        "        # self.fc2 = nn.Linear(2048,1024)\n",
        "        # self.fc3 = nn.Linear(1024,512)\n",
        "        self.fc4 = nn.Linear(500,133)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ## Define forward behavior\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv1(x)))# 3*256*256 - 16*256*256 - 16*128*128\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv2(x)))# 16*128*128 - 64*128*128 - 64*64*64\n",
        "        #print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv3(x)))# 64*64*64 - 128*64*64 - 128*32*32\n",
        "        #print(x.shape)\n",
        "        #x = self.pool(F.relu(self.conv4(x)))# 128*32*32 - 256*32*32 - 256*16*16\n",
        "        #print(x.shape)\n",
        "        \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1,128*28*28)\n",
        "        #print(x.shape)\n",
        "        # x = F.relu(self.fc1(x))\n",
        "        # #print(x.shape)\n",
        "        # x = self.dropout(x)\n",
        "        # #print(x.shape)\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        # x = self.dropout(x)\n",
        "        # x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
        "\n",
        "# instantiate the CNN\n",
        "model_scratch = Net()\n",
        "print(model_scratch)\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    model_scratch.cuda()\n",
        "# model_scratch.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=500, bias=True)\n",
            "  (fc4): Linear(in_features=500, out_features=133, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OyM73w7axsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchsummary import summary\n",
        "# summary(model_scratch, input_size=(3, 256, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ddVtLyAaRC",
        "colab_type": "text"
      },
      "source": [
        "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak4e1DNgAaRD",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VytVkI8WAaRE",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and the optimizer as `optimizer_scratch` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjORlwnsAaRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "### TODO: select loss function\n",
        "criterion_scratch = nn.CrossEntropyLoss()\n",
        "\n",
        "### TODO: select optimizer\n",
        "optimizer_scratch = optim.SGD(model_scratch.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbgMrKw6AaRH",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_scratch.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cwk5tiCAaRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following import is required for training to be robust to truncated images\n",
        "from PIL import ImageFile\n",
        "from torch.autograd import Variable\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# loaders_scratch[\"train\"] = train_loader\n",
        "# loaders_scratch[\"test\"] = test_loader\n",
        "# loaders_scratch[\"valid\"] = valid_loader\n",
        "\n",
        "def train(n_epochs, train_loader, valid_loader, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            ## record the average training loss, using something like\n",
        "            # data = (data)\n",
        "            # target = (target)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output,target)\n",
        "            loss.backward()\n",
        "            #train_loss+= loss.item()*data.size(0)\n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## update the average validation loss\n",
        "            # data = (data)\n",
        "            # target = (target)\n",
        "            output = model(data)\n",
        "            loss = criterion(output,target)\n",
        "            #valid_loss += loss.item()*data.size(0)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "          valid_loss_min,\n",
        "          valid_loss))\n",
        "          torch.save(model.state_dict(), 'model_augmented.pt')\n",
        "          valid_loss_min = valid_loss\n",
        "    # return trained model\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkIbihRhVpWT",
        "colab_type": "code",
        "outputId": "4129d3b2-eaf2-4a1a-c8ec-60135497133f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#train the model\n",
        "model_scratch = train(20, train_loader, valid_loader,model_scratch, optimizer_scratch, \n",
        "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 4.890217 \tValidation Loss: 4.890456\n",
            "Validation loss decreased (inf --> 4.890456).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 4.890158 \tValidation Loss: 4.889677\n",
            "Validation loss decreased (4.890456 --> 4.889677).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 4.890130 \tValidation Loss: 4.889912\n",
            "Epoch: 4 \tTraining Loss: 4.890437 \tValidation Loss: 4.890250\n",
            "Epoch: 5 \tTraining Loss: 4.889900 \tValidation Loss: 4.889756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-db6d299c9ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_scratch = train(20, train_loader, valid_loader,model_scratch, optimizer_scratch, \n\u001b[0;32m----> 2\u001b[0;31m                       criterion_scratch, use_cuda, 'model_scratch.pt')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-0f99b839ddfd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, train_loader, valid_loader, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m# move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \"\"\"\n\u001b[1;32m    686\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresized_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresized_crop\u001b[0;34m(img, i, j, h, w, size, interpolation)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img should be PIL Image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYLSgXasVr89",
        "colab_type": "code",
        "outputId": "f1bbfd8b-5eb0-4421-bd80-1c4b8389c3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# load the model that got the best validation accuracy\n",
        "model_scratch.load_state_dict(torch.load('model_scratch.pt'))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f83fe6a7a736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_scratch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_scratch.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_scratch.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJVGYuLoAaRN",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NROlqff4AaRP",
        "colab_type": "code",
        "outputId": "f728d47b-f447-40c1-8e0b-e00b713d2037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def test(test_loader, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "test(test_loader,model_scratch, criterion_scratch, use_cuda)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.890076\n",
            "\n",
            "\n",
            "Test Accuracy:  0% ( 6/836)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5HN9QGGAaRT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step4'></a>\n",
        "## Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
        "\n",
        "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
        "\n",
        "### (IMPLEMENTATION) Specify Data Loaders for the Dog Dataset\n",
        "\n",
        "Use the code cell below to write three separate [data loaders](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) for the training, validation, and test datasets of dog images (located at `dogImages/train`, `dogImages/valid`, and `dogImages/test`, respectively). \n",
        "\n",
        "If you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gODEMUf5AaRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1fb535d2-cda8-40ee-9b90-b45ec31d6692"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.datasets\n",
        "from torchvision import transforms,utils\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "BATCH_SIZE = 20\n",
        "NUM_WORKERS = 0\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                transforms.RandomRotation(15),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder('/content/dog_images/train',transform=transform)\n",
        "valid_data = torchvision.datasets.ImageFolder('/content/dog_images/valid',transform=transform)\n",
        "test_data = torchvision.datasets.ImageFolder('/content/dog_images/test',transform=transform)\n",
        "\n",
        "print(\"Number of train data : \",str(len(train_data)))\n",
        "print(\"Number of valid data : \",str(len(valid_data)))\n",
        "print(\"Number of test data : \",str(len(test_data)))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS,shuffle=True)\n",
        "\n",
        "loader_transfer = {'train':train_loader,\n",
        "                   'valid':valid_loader,\n",
        "                   'test':test_loader}\n",
        "\n",
        "use_cuda                   "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train data :  6680\n",
            "Number of valid data :  835\n",
            "Number of test data :  836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMn-yxjoAaRX",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXydHC32AaRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "c6504e33-937f-465c-ec54-98cb9d1d5666"
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "## TODO: Specify model architecture \n",
        "model_transfer = models.vgg16(pretrained = True)\n",
        "\n",
        "for param in model_transfer.features.parameters():\n",
        "  param.require_grad = False\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer = model_transfer.cuda()\n",
        "\n",
        "print(model_transfer)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alcH-V60eL0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_num = model_transfer.classifier[0].in_features\n",
        "\n",
        "transfer_fc1 = nn.Linear(input_num,500)\n",
        "transfer_fc2 = nn.Linear(500,250)\n",
        "transfer_fc3 = nn.Linear(250,133)\n",
        "\n",
        "model_transfer.classifier[0] = transfer_fc1\n",
        "model_transfer.classifier[3] = transfer_fc2\n",
        "model_transfer.classifier[6] = transfer_fc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgY9cT8pAaRb",
        "colab_type": "text"
      },
      "source": [
        "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOdR7WeLAaRd",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoTDTKS6AaRe",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSLQP9N5AaRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "optimizer_transfer = optim.SGD(model_transfer.classifier.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUDDB6VMAaRh",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL6MEDoii41s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_transfer = model_transfer.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJScWrlKAaRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "78cf69b3-d32e-45b2-8dee-43a97bef8557"
      },
      "source": [
        "# train the model\n",
        "n_epochs = 20\n",
        "model_transfer = train(n_epochs, loader_transfer['train'],loader_transfer['valid'], model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 4.932384 \tValidation Loss: 4.903216\n",
            "Validation loss decreased (inf --> 4.903216).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 4.932380 \tValidation Loss: 4.897895\n",
            "Validation loss decreased (4.903216 --> 4.897895).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 4.932011 \tValidation Loss: 4.899052\n",
            "Epoch: 4 \tTraining Loss: 4.924120 \tValidation Loss: 4.891762\n",
            "Validation loss decreased (4.897895 --> 4.891762).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 4.926314 \tValidation Loss: 4.896430\n",
            "Epoch: 6 \tTraining Loss: 4.933452 \tValidation Loss: 4.893505\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-6615b36f7040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_transfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_transfer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloader_transfer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_transfer.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-0f99b839ddfd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, train_loader, valid_loader, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m## find the loss and update the model parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m## record the average training loss, using something like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J25CH9DmdlHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model that got the best validation accuracy (uncomment the line below)\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1WkVaoPAaRj",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYPlHtRcAaRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c102294d-4966-4794-99d1-6f32c7fc74b6"
      },
      "source": [
        "test(loader_transfer['test'], model_transfer, criterion_transfer, use_cuda)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 4.900874\n",
            "\n",
            "\n",
            "Test Accuracy:  1% (11/836)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUp7vURiAaRl",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
        "\n",
        "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan hound`, etc) that is predicted by your model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAWEWSO9AaRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO: Write a function that takes a path to an image as input\n",
        "### and returns the dog breed that is predicted by the model.\n",
        "\n",
        "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
        "# class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
        "#class_names = [str(name)[4:].replace(\"_\", \" \") for name in next(os.walk('content/dog_images/train/'))[1]]\n",
        "class_names = [item[4:].replace(\"_\", \" \") for item in loader_transfer['train'].dataset.classes]\n",
        "def predict_breed_transfer(img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "    img = transform(img)\n",
        "    img = torch.unsqueeze(img,0)\n",
        "    if use_cuda:\n",
        "      img = img.cuda()\n",
        "    val,index = torch.max(model_transfer(img),1)\n",
        "    return int(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4XyEmlLAaRn",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step5'></a>\n",
        "## Step 5: Write your Algorithm\n",
        "\n",
        "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
        "- if a __dog__ is detected in the image, return the predicted breed.\n",
        "- if a __human__ is detected in the image, return the resembling dog breed.\n",
        "- if __neither__ is detected in the image, provide output that indicates an error.\n",
        "\n",
        "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 4 to predict dog breed.  \n",
        "\n",
        "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
        "\n",
        "![Sample Human Output](images/sample_human_output.png)\n",
        "\n",
        "\n",
        "### (IMPLEMENTATION) Write your Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCHSaQwAaRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO: Write your algorithm.\n",
        "### Feel free to use as many code cells as needed.\n",
        "def img_preprocess(img_path):\n",
        "  img = Image.open(img_path)\n",
        "  transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n",
        "  img = transform(img)\n",
        "  img = torch.unsqueeze(img,0)\n",
        "  if use_cuda:\n",
        "    img = img.cuda()\n",
        "  return img\n",
        "\n",
        "def run_app(img_path):\n",
        "    ## handle cases for a human face, dog, and neither\n",
        "    new_img = img_preprocess(img_path)\n",
        "    if dog_detector(img_path):\n",
        "      val,index = torch.max(model_transfer(new_img),1)\n",
        "      index = index.data.cpu().numpy()[0]\n",
        "      breed = class_names[index]\n",
        "      print(\"The Dog breed is \",str(breed))\n",
        "    elif face_detector(img_path):\n",
        "      print(\"Human face detected... \\n\")\n",
        "      out = model_transfer(new_img)\n",
        "      val,index = torch.max(out,1)\n",
        "      index = index.data.cpu().numpy()[0]\n",
        "      breed = class_names[index]\n",
        "      print(\"The prediction is \",str(index))\n",
        "      print(\"Similar to Dog breed \",str(breed))\n",
        "    else:\n",
        "      print(\"Error: No Dog or Human face detected\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NYohxUKAaRq",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step6'></a>\n",
        "## Step 6: Test Your Algorithm\n",
        "\n",
        "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that _you_ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
        "\n",
        "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
        "\n",
        "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
        "\n",
        "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCU17Z7CAaRr",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ (Three possible points for improvement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbqvPQg5AaRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e954e10-d763-474c-8b94-2a2e44c0b9bb"
      },
      "source": [
        "## TODO: Execute your algorithm from Step 6 on\n",
        "## at least 6 images on your computer.\n",
        "## Feel free to use as many code cells as needed.\n",
        "\n",
        "## suggested code, below\n",
        "for file in np.hstack((human_files[:3], dog_files[:3])):\n",
        "    run_app(file)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-82e5fa3ead0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdog_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-3731942cb040>\u001b[0m in \u001b[0;36mrun_app\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m## handle cases for a human face, dog, and neither\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdog_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-e0b33f4f3259>\u001b[0m in \u001b[0;36mdog_detector\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdog_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m## TODO: Complete the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculation of the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m151\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m268\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Checking if its Dog/Human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-273b5e64eb5c>\u001b[0m in \u001b[0;36mVGG16_predict\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# OPEN IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TRANSFORM THE IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# PREDICT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# prob = torch.exp(output)  # CALCULATE THE PROBABILITIES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# top_prob,top_class = prob.topk(1,dim=1) # FIND THE BEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oy531DGuwGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}